<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>My Master Presentation</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- My css -->
		<link rel="stylesheet" href="css/main.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h2>Hybrid classification method for imbalanced datasets</h1>
					<br>
					<p>Tianxiang Gao</p>
					<p>
						<small>August 20, 2015</a></small>
					</p>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li class="fragment highlight-blue">Motivation</li>
						<li>Introduction</li>
						<li>Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<!-- Example of nested vertical slides -->

				<section>
					<section>
						<h2>Motivation</h2>
						<p>“Learning from Imbalanced Data Sets,” <a href="http://www.site.uottawa.ca/~nat/Workshop2000/workshop2000.html" target="_blank">Proc. Am. Assoc. for Artificial Intelligence (AAAI) Workshop</a>, N. Japkowicz, ed., 2000, (Technical Report WS-00-05).</p>
						<p>“Workshop Learning from Imbalanced Data Sets II,” <a href="https://www.site.uottawa.ca/~nat/Workshop2003/workshop2003.html" target="_blank">International Conference on Machine Learning (ICML)</a>, N.V. Chawla, N. Japkowicz, and A. Kolcz, eds., 2003.</p>
						<p>N.V. Chawla, N. Japkowicz, and A. Kolcz, “Editorial: Special Issue on Learning from Imbalanced Data Sets,” <a href="https://www.site.uottawa.ca/~nat/Research/explorations_cfp.html" target="_blank">The ACM Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD)  Explorations Newsletter</a>, vol. 6, no. 1, pp. 1-6, 2004.</p>
						<br>
					</section>
					<section>
						<h2>fraud/intrusion detection</h2>
						<img data-src="img/Credit-Card-Fraud.jpg">
					</section>
					<section>
						<h2>medical diagnosis/monitoring</h2>
						<img data-src="img/medical-diagnosis.jpg">
					</section>
					<section>
						<h2>bioinformatics</h2>
						<img data-src="img/bioinformatics.jpg">
					</section>
					<section>
						<h2>direct marketing</h2>
						<img data-src="img/direct-marketing.png">
					</section>
					<section>
						<h2>Nature of problem</h2>
						<img height="450px" data-src="img/problems.png">
					</section>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li class="fragment highlight-blue">Introduction</li>
						<li>Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li><span style="color:#1b91ff;">Introduction </span>
							<ul>
								<li class="fragment highlight-blue">Classifier - Decision tree</li>
								<li>Evaluate performance</li>
								<li>Nature of problem</li>
							</ul>
						</li>
						<li>Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Classifier</h2>
					<p class="fragment highlight-current-blue"><strong>Classification problem</strong> is to correctly classifiy the previously unseen <em>testing dataset</em> based on the given <em>training dataset</em>. We deal with <em>binary</em> cases, positive class and negative class.</p>
					<p class="fragment highlight-current-blue">An algorithm that implements classification is known as a <strong>Classifier</strong>.</p>
					<p class="fragment highlight-current-blue"><strong>Decision tree</strong> is a tree-like classifier.</p>
				</section>

				<section>
					<section>
						<h2>Dataset of Playing Tennis</h2>
						<table>
							<thead>
								<tr>
									<th>Outlook</th>
									<th>Temp.</th>
									<th>Humidity</th>
									<th>Windy</th>
									<th class="response-variable">Play</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Sunny</td>
									<td>Hot</td>
									<td>High</td>
									<td>False</td>
									<td class="response-variable">No</td>
								<tr>
								<tr>
									<td>Sunny</td>
									<td>Hot</td>
									<td>High</td>
									<td>True</td>
									<td class="response-variable">No</td>
								<tr>
								<tr>
									<td>Overcase</td>
									<td>Hot</td>
									<td>High</td>
									<td>False</td>
									<td class="response-variable">Yes</td>
								<tr>
								<tr>
									<td>...</td>
									<td>...</td>
									<td>...</td>
									<td>...</td>
									<td class="response-variable">...</td>
								<tr>
								<tr>
									<td>Rainy</td>
									<td>Mild</td>
									<td>High</td>
									<td>True</td>
									<td class="response-variable">No</td>
								<tr>
							</tbody>
						</table>
					</section>

					<section>
						<h2>Decision Tree</h2>
						<img height="500px" data-src="img/weather_example.png">
						<p style="text-align:right;"> -Matt Tanner</p>
					</section>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li><span style="color:#1b91ff;">Introduction</span>
							<ul>
								<li>Classifier - Decision tree</li>
								<li class="fragment highlight-blue">Evaluate performace</li>
								<li>Nature of problem</li>
							</ul>
						</li>
						<li>Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Why?</h2>
					<ul>
						<li>Multiple classifiers are available</li>
						<li>For each classifier, multiple choices are available for settings</li>
						<li>To choose best classifier</li>
					</ul>
				</section>

				<section>
					<h2>Cutoff value</h2>
					<p>Most algorithms classify via a 2-steps process:</p>
					<ol>
						<li>Compute probability of belonging to $positive$ class.</li>
						<li>Compare to cutoff value, and classify accordingly.</li>
					</ol>
					<p>Default cutoff value is <em>0.5</em>
					<ul>
						<li>If >= 0.5, classify as $positive$.</li>
						<li>If &lt; 0.5, classify as $negative$.</li>
					</ul>
					</p>
				</section>
				
				<section>
					<h2>Confusion Matrix</h2>
					<table>
						<thead>
							<tr>
								<th></th>
								<th>Predicted +</th>
								<th>Predicted -</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<th>Actual +</th>
								<td>True Positive (TP)</td>
								<td>False Negative (FN)</td>
							<tr>
							<tr>
								<th>Actual -</th>
								<td>False Positive (FP)</td>
								<td>True Negative (TN)</td>
							<tr>
						</tbody>
					</table>
				</section>

				<section>
					<h2>Accuracy</h2>
					<p>$ ACC = \frac{TP+TN}{TP + TN + FN + FP}$</p>
					<p>May not very useful if imbalanced datasets.</p>
				</section>

				<section>
					<h2>TPR & FPR</h2>
					<p>True positive rate (TPR) $= \frac{TP}{TP + FN} $</p>
					<p>False positive rate (FPR) $= \frac{FP}{FP + TN} $</p>
				</section>

				<section>
					<h2>ROC curve</h2>
					<blockquote cite="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">In statistics, a receiver operating characteristic (ROC), or ROC curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate against the false positive rate at various threshold settings.
					</blockquote>
					<p style="float:right;">-Wikipedia</p>
				</section>

				<section>
					<h2>ROC &amp; AUC</h2>
					<img data-src="http://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png" style="height:400px;">
					<p><strong>Area Under the ROC curve (AUC)</strong> is to quantify the overal performance of a classifier.</p>
				</section>

				<section>
					<section>
						<h2>Class Balance Accuracy</h2>
						$CBA =\frac{\sum_{i=1}^{k} \frac{C_{ii}}{max(C_{i.}, C_{.i}) } }{k}$
						<p>where $C_{i.} = \sum_{j=1}^{k} C_{ij} $ and $C_{.i} = \sum_{i=1}^{k} C_{ji} $. </p>
						<br>
						<p style="text-align:right;"> -L. Mosley and S. Olafsson 2013.</p>
					</section>

					<section>
						<h2>Confusion Matrix</h2>
						<table>
							<thead>
								<tr>
									<th></th>
									<th>Predicted +</th>
									<th>Predicted -</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<th>Actual +</th>
									<td class="fragment highlight-blue">True Positive (TP)</td>
									<td class="fragment highlight-current-blue">False Negative (FN)</td>
								<tr>
								<tr>
									<th>Actual -</th>
									<td class="fragment highlight-current-blue">False Positive (FP)</td>
									<td class="fragment highlight-red">True Negative (TN)</td>
								<tr>
							</tbody>
						</table>
					</section>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li><span style="color:#1b91ff;">Introduction</span>
							<ul>
								<li>Classifier - Decision tree</li>
								<li>Evaluate performance</li>
								<li class="fragment highlight-blue">Nature of problem</li>
							</ul>
						</li>
						<li>Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Nature of problem</h2>
					<img height="450px" data-src="img/problems.png">
					<p><span class="fragment highlight-current-blue">Overlap,</span> <span class="fragment highlight-current-blue">within-class imbalance,</span> <span class="fragment highlight-current-blue"> disjunct rules,</span> <span class="fragment highlight-current-blue">authenticity of data.</span></p>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li class="fragment highlight-blue">Methodology</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li><span style="color:#1b91ff;">Methodology</span>
							<ul>
								<li class="fragment highlight-blue">Sampling</li>
								<li>Instance Selection</li>
								<li>Hybrid method</li>
							</ul>

						</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Sampling</h2>
					<ul>
						<li class="fragment highlight-current-blue">Undersampling</li>
						<li class="fragment highlight-current-blue">Oversampling (with replacement)</li>
						<li>SMOTE</li>
					</ul>
				</section>

				<section>
					<blockquote>Under-sampling the majority class enables <strong>better</strong> classifiers to be built than over-sampling the minority class.
					<br>
					<br>
					If replicate the minority class, the decision region for the minority class becomes very <strong>specific</strong> and will cause new splits in the decision tree...in essence, <strong>overfitting</strong>.
					<br>
					<br>
					Replication of the minority class does not cause its decision boundary to spread into the majority class region.
					</blockquote>
					<p style="float:right;"> - Chawla, Nitesh V., et al 2002.</p>
				</section>

				<section>
					<section>
						<h2>SMOTE</h2>
						<p>SMOTE stands for Synthetic Minority Over-sampling Technique</p>
						<p style="text-align:right;"> - Chawla, Nitesh V., et al 2002.</p>
					</section>

					<section>
						<h2>SMOTE</h2>
						<img data-src="img/SMOTE.png" style="max-height:550px;">
						<p style="text-align:right; font-size: 20px"> - He, Haibo, <i>Learning from imbalanced datasets</i> 2009</p>
						<p>Continuous - $x_{new}=x_i+(\hat{x}_i-x_i)\times \delta$</p>
						<p>Categorical - $x_{new}=majorityVote(x_i)$</p>

					</section>

				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li><span style="color:#1b91ff;">Methodology</span>
							<ul>
								<li>Sampling</li>
								<li class="fragment highlight-blue">Instance Selection</li>
								<li>Hybrid method</li>
							</ul>

						</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Instance Selection</h2>
					<p>Selects subset of training dataset such that removes superfluous instances, maintain performances.
					</p>
				</section>

				<section>
					<h2> Greedy Selection</h2>
					<p>Greedy selection is a two-steps wrapper method:<span class="fragment highlight-current-blue">generates a number of candidate subsets,</span> <span class="fragment highlight-current-blue"> and starts with one candidate subset</span> <span class="fragment highlight-current-blue">and continuouly combines the other subsets</span> <span class="fragment highlight-current-blue">if combining improves the performance of classifier.</span>
					</p>
					<br>
					<p style="text-align:right;"> - W. Bennette and S. Olafsson 2013.</p>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li><span style="color:#1b91ff;">Methodology</span>
							<ul>
								<li>Sampling</li>
								<li>Instance Selection</li>
								<li class="fragment highlight-blue">Hybrid method</li>
							</ul>

						</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Hybrid Method</h2>
					
					<ul>
						<li>Hyrid method is a <strong>combination</strong> method of SMOTE and greedy selection.</p></li>
						<li>
							<ol>
								<li class="fragment highlight-current-blue">Generate synthetic instances for minority class, and combines those synthetic instances with majority instances.</li>
								<li class="fragment highlight-current-blue">Select the ideal subset from the SMOTEd instances by using greedy selection as the final training dataset.</li>
							</ol>
						</li>
					</ul>
				</section>

				<section>
					<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li>Methodology</li>
						<li><span style="color:#1b91ff;">Experimental results</span>
							<ul>
								<li class="fragment highlight-blue">Characteristics of Datasets</li>
								<li>Results</li>
							</ul>
						</li>
						<li>Conclusion</li>
					</ul>
				</section>

				<section>
					<h2>Characteristics of Datasets</h2>
					<p>4 well-known imbalanced datasets in UCI machine learning repository, and one medical dataset</p>
					<img data-src="img/data.png">
					<p style="text-align:right; font-size: 20px"> - Chawla, Nitesh V. 2002. </p>
					<p style="text-align:right; font-size: 20px"> - Gang Wu 2003.</p>
					<p style="text-align:right; font-size: 20px"> - Nathalie Japkowicz 2004.</p>
					<p style="text-align:right; font-size: 20px"> - Haibo He 2009.</p>
				</section>
					
				<section>
				<h2>Contents</h2>
					<ul>
						<li>Motivation</li>
						<li>Introduction</li>
						<li>Methodology</li>
						<li><span style="color:#1b91ff;">Experimental results</span>
							<ul>
								<li>Characteristics of Datasets</li>
								<li class="fragment highlight-blue">Results</li>
							</ul>
						</li>
						<li>Conclusion</li>
					</ul>
				</section>
						
				<section>
					<h2>Results</h2>
					<p class="fragment highlight-current-blue">Randomly select <strong>4/5</strong> of a dataset as original training dataset, and the rest is testing dataset.</p>
					<p class="fragment highlight-current-blue">Implement those strategies to preprocess the dataset and we got four different training datasets: <strong>Control, Greedy Selection, SMOTE, and Hybrid</strong>.</p>
					<p class="fragment highlight-current-blue">Fit those four different training datasets through regular <strong>decision tree</strong>.</p>
					<p class="fragment highlight-current-blue">Predict the test dataset.</p>
					<p class="fragment highlight-current-blue">Process these steps over <strong>100</strong> times among each dataset. Then, evaluate predications through computing <strong>AUC, CBA, and ACC</strong>.</p>
				</section>

				<section>
					<section>
						<h2 >Dataset: G7</h4>
						<img data-src="img/g7_cba.png">
						<img data-src="img/g7_auc.png">	
					</section>

					<section>
						<h2>Dataset: car3</h2>
						<img data-src="img/car3_cba.png">
						<img data-src="img/car3_auc.png">	
					</section>
					
					<section>
						<h2>Dataset: yeast5</h2>
						<img data-src="img/yeast5_cba.png">
						<img data-src="img/yeast5_auc.png">	
					</section>

					<section>
						<h2>Dataset: medical</h2>
						<img data-src="img/medical_cba.png">
						<img data-src="img/medical_auc.png">	
					</section>
					
				</section>

				<section>
					<section>
						<h2>Analysis of Hybrid Method</h2>
						<img width="400" data-src="img/23dataset.png" style="float:left;">
						<img width="400" data-src="img/23minority.png" style="float:right;">
					</section>

					<section>
						<h2>Analysis of Hybrid Method</h2>
						<img width="310" data-src="img/23smote_area_9.png" style="float:left;">
						<img width="310" data-src="img/23smote_under_area_9.png">
						<img width="310" data-src="img/23hybrid_area_9.png" style="float:right;">
					</section>
				</section>

				<section>
					<h2>Contents</h2>
						<ul>
							<li>Motivation</li>
							<li>Introduction</li>
							<li>Methodology</li>
							<li>Experimental results</li>
							<li style="color:#1b91ff;">Conclusion</li>
						</ul>
				</section>

				<section>
					<h2>Conclusion</h2>
					<ul>
						<li>Hybrid Classification method makes decision tree works better.</li>
						<li>Robust</li>
						<li>Selecting an appropriate assessment metric is essential to wrapper-based method.</li>
						<li>Comprehensive assessment metric works better than non-comprehensive.</li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h1>Questions?</h1>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				transition: 'convex', // none/fade/slide/convex/concave/zoom

				// math

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]

			});
		</script>



	</body>
</html>
